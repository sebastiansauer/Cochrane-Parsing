---
title: "01-Cochrane-Scraping"
ipsum_meta:
  twitter_card: "Summary info for the Twitter Card"
  twitter_site: "\\@sitehandle"
  twitter_creator: "\\@creatorhandle"
  og_url: "https\\://example.com/open/graph/finalURLfor/this"
  og_description: "A modest size description of the content"
  og_image: "https\\://example.com/open/graph/imageURLfor/this"
output: 
  hrbrthemes::ipsum:
    toc: true
    number_section: TRUE
editor_options: 
  chunk_output_type: console
---


# Setup

```{r include=FALSE}
knitr::opts_chunk$set(fig.retina = 2)
```

```{r ipsum_setup, message=FALSE, warning=FALSE, cache=FALSE, echo=FALSE}
library(hrbrthemes)
library(tidyverse)  # data wrangling
library(rvest)  # web scraping
library(xml2)  # web scraping
library(stringr)  # string manipulation
library(printr)  # print dfs as tables
library(glue)  # glueing
library(rcrossref)  # citation count
library(conflicted)
library(glue)
library(polite)
library(tidytext)
library(readxl)
update_geom_font_defaults(font_rc)
```


Declare conflict preferences:

```{r}
conflict_prefer("filter", "dplyr")
```



# Define pages to be scraped


```{r}
dois_list <- readxl::read_xlsx("data/doi-list-reviewer.xlsx")
```



## Check
 
```{r}
dois_list %>% 
  count(reviewer)
```




## Subsamples of reviewers


```{r}
dois_jh <- dois_list %>% 
  filter(reviewer == "JH")

dois_hw <- dois_list %>% 
  filter(reviewer == "HW") %>% 
  pull(url)
```



## Get sample for each reviewer

```{r}
dois_by_reviewer_sample <- 
  dois_list %>% 
  group_by(reviewer) %>% 
  slice(1:50) %>% 
  drop_na()
```


```{r}
dois_by_reviewer_sample %>% 
  count(reviewer)
```



## URLs for checking

```{r}
reviews_ss1 <- "http://dx.doi.org/10.1002/14651858.CD001180.pub4"


# mindfulness open access review on intervention:
# friendly review:
mindfulness_url <- "https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD012791.pub2/full"  # 2 sof tables


cochrane_url <- dois_reviews_paywalled[1]

# paywalled url:
paywalled_url <- "https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD013879/full"

# friendly review:
friendly_url <- "http://dx.doi.org/10.1002/14651858.CD003474.pub4"

# no "summary of findings table"
nosumtable_url <- "https://doi.org/10.1002/14651858.CD009326.pub3"


# not recent review:
old_version_url <- "https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD001352/full"


url_throws_error <- "http://dx.doi.org/10.1002/14651858.CD000245.pub4"

url_withdrawn <- "https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD004125.pub3/full"

url_jh1 <- dois_jh[1]

review_url <- prob_url 

url_np <- "https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD012137.pub2/full"

url_404 <- "https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD007701.pub4/full"

bad_url <- "https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD012137.pub2/full"

prob_url <- "https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD009099.pub3"
```




# Source functions

```{r}
source("funs/helper-funs.R")
source("funs/funs-parse-cochrane.R")
source("funs/get_summary_table.R")
source("funs/parse-review-parts.R")
```







# Test run: parse one review

```{r}
review <- list()
  
  review$page_content <- read_html(review_url)  
  review$metadata <- get_review_metadata(review$page_content)
  review$abstract <- get_abstract(review$page_content)
  
  
  #debug(get_nr_of_summary_tables)
  
  #undebug(get_nr_of_summary_tables)
  review$number_of_summary_tables <- get_nr_of_summary_tables(review$page_content)
  
  
  #debug(get_summary_table)
   
  review$summarytable1 <- get_summary_table(review$page_content)
  review$summaryTable_metadata <- get_summary_table_metadata(review$page_content)
  
  #undebug(add_metadata_to_summary_table)
  review$final_table <- concat_tables(
    page_content = review$page_content,
    summarytable = review$summarytable1,
    metadata_review =  review$metadata,
    abstract_review = review$abstract,
    metadata_summaryTable = review$summaryTable_metadata)
```




## init

```{r}
count_reviews <- 1
table_number <- 1
overwrite <- FALSE
verbose <- TRUE
reviewer <- "ss"
overwrite_file <- TRUE
init_new_review()
output_dir <- "output"
file_type = "csv"
```





## Test it

```{r test-parse-review}
test_review <- parse_review(prob_url, 
                            overwrite = TRUE,
                            reviewer = "?",
                            output_dir_to_write = "output")
test_review <- parse_review(review_url, overwrite = TRUE)
test_review <- parse_review(mindfulness_url, overwrite = TRUE)
```








# Parse reviews









## Loop


### all



```{r}
count_reviews <- 1

reviews_all_50 <- 
  map2_dfr(
    .x = dois_by_reviewer_sample$url,
    .y = dois_by_reviewer_sample$reviewer,
    .f = ~ parse_review(review_url = .x,
                        reviewer = .y,
                        overwrite = TRUE,
                        output_dir = glue("output/{.y}")))


writexl::write_xlsx(reviews_all_50, path = "output/all_reviewers/all_reviewers_50.xlsx")
```


```{r}
reviews_all_50 %>% 
  count(reviewer)
```



#### Read from disk

```{r}
reviews_all_50 <- 
  read_xlsx("output/all_reviewers/all_reviewers_50.xlsx")
```



#### Save in parts (per reviewer)

```{r}

reviewers <- 
reviews_all_50 %>% 
  select(reviewer) %>% 
  distinct(reviewer) %>% 
  pull(reviewer)


reviewers %>% 
  walk( ~ write_csv(
    reviewers_all_50 %>% filter(reviewer == .),
    file = glue("output/all_reviewers/sample_50_{.}.csv")))

```




### JH


```{r}
dois_jh1 <-
  dois_jh %>% 
  slice(80:100) %>% 
  pull(url) 

review_jh1 <- 
  dois_jh1 %>% 
  map_dfr(parse_review, .id = "id_review", output_dir = "output/jh",
          reviewer = "jh",
          overwrite_file = FALSE)
```



```{r}
writexl::write_xlsx(review_jh1, path = "output/reviews_jh1.xlsx")
```


### NP


#### NP1
```{r}
dois_np1 <- 
  dois_by_reviewer_sample %>% 
  filter(str_detect(reviewer, "NP")) %>% 
  slice(5:50) %>% 
  pull(url)

review_np1 <- 
  dois_np1 %>% 
  map_dfr(parse_review, .id = "id_review", output_dir = "output/np",
          overwrite_file = FALSE,
          reviewer = "np")
```

save to disk:

```{r}
writexl::write_xlsx(review_np1, path = "output/np/reviews_np1.xlsx")
```



#### NP2

```{r}
dois_np2 <- 
  read_csv("data/doi-list-np_1.csv")
```


```{r}
reviews_np2 <- 
  dois_np2 %>% 
  pull(url) %>% 
  map_dfr(parse_review, .id = "id_review", output_dir = "output/np",
          overwrite_file = FALSE,
          reviewer = "np")


writexl::write_xlsx(reviews_np2, path = "output/np/reviews_np2.xlsx")
```




### HW

```{r}
reviews_parsed_hw <-
  dois_hw %>% 
  magrittr::extract(49:50) %>% 
  map_dfr(parse_review, output_dir = "output/hw")
```

```{r}
reviews_parsed_hw 
```

```{r}
writexl::write_xlsx(reviews_parsed_hw, path = "output/hw/reviews_hw.xlsx")
```



### ss



```{r}
dois_ss <- 
  dois_by_reviewer_sample %>% 
  filter(str_detect(reviewer, "SS")) %>% 
  #slice(5:50) %>% 
  pull(url)

count_reviews <- 1
reviews_ss <- 
  c("http://dx.doi.org/10.1002/14651858.CD009110.pub3", 
    prob_url) %>% 
  map_dfr(parse_review, 
          .id = "id_review", 
          output_dir = "output/ss",
          overwrite_file = TRUE,
          reviewer = "ss")





```



```{r}
writexl::write_xlsx(review_ss, path = "output/ss/reviews_ss.xlsx")
```







```{r}
reviews_parsed4 <- 
   dois_reviews_2017[43:50] %>% 
  # c(nosumtable_url, mindfulness_url, old_version_url) %>% 
   #old_version_url %>% 
   map_dfr(parse_review, .id = "id_review")
```



# Convert to data frame


```{r}
reviews_parsed2 <- 
   reviews_parsed %>% 
   mutate(review_id = duplicated(doi)) %>% 
   select(review_id, doi, everything())
```




# Save to disk


```{r}
write_csv(reviews_parsed3, 
            file = paste0("test_reviews_infos3", ".csv"))

writexl::write_xlsx(reviews_parsed3,
                    path = "test_reviews_infos3.xlsx")

writexl::write_xlsx(reviews_ss,
                    path = "output/ss/reviews_ss.xlsx")
write_csv(reviews_ss, 
            file = "output/ss/reviews_ss.csv")

```






```{r bib, include=FALSE}
# KEEP THIS AT THE END OF THE DOCUMENT TO GENERATE A LOCAL bib FILE FOR PKGS USED
knitr::write_bib(sub("^package:", "", grep("package", search(), value=TRUE)), file='skeleton.bib')
```